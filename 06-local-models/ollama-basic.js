import ollama from "ollama";

async function basicChat() {
  console.log("üöÄ Ollama Basic Demo\n");
  console.log("=".repeat(60));

  console.log("üí° Instalace Ollama:");
  console.log("   brew install ollama");
  console.log("   ollama pull llama3.1");
  console.log("   ollama pull codellama");
  console.log("   ollama pull qwen2.5-coder\n");

  try {
    console.log("ü§ñ Chatting with Llama 3.1...\n");

    const response = await ollama.chat({
      model: "llama3.1",
      messages: [
        {
          role: "system",
          content:
            "You are an expert in JavaScript and modern web development.",
        },
        {
          role: "user",
          content:
            "Explain the difference between Promise.all and Promise.allSettled",
        },
      ],
    });

    console.log("Response:", response.message.content);
    console.log("\nüìä Metadata:");
    console.log(`   Model: ${response.model}`);
    console.log(
      `   Total duration: ${(response.total_duration / 1e9).toFixed(2)}s`,
    );
    console.log(
      `   Load duration: ${(response.load_duration / 1e9).toFixed(2)}s`,
    );
  } catch (error) {
    console.log("‚ùå Ollama nen√≠ spu≈°tƒõn√Ω nebo model nen√≠ sta≈æen√Ω");
    console.log("   Spus≈•: ollama serve");
    console.log("   St√°hni model: ollama pull llama3.1");
  }
}

async function streamingChat() {
  console.log("\n\n" + "=".repeat(60));
  console.log("üì° Streaming Response\n");

  try {
    const stream = await ollama.chat({
      model: "llama3.1",
      messages: [
        {
          role: "user",
          content: "Write a debounce function in JavaScript",
        },
      ],
      stream: true,
    });

    process.stdout.write("Response: ");

    for await (const chunk of stream) {
      process.stdout.write(chunk.message.content);
    }

    console.log("\n");
  } catch (error) {
    console.log("‚ùå Streaming demo vy≈æaduje bƒõ≈æ√≠c√≠ Ollama");
  }
}

async function codeGeneration() {
  console.log("\n" + "=".repeat(60));
  console.log("üíª Code Generation with CodeLlama\n");

  try {
    const response = await ollama.chat({
      model: "codellama",
      messages: [
        {
          role: "user",
          content: `Write a TypeScript function for email validation with these requirements:
- Type-safe
- Returns boolean
- Validates format
- M√° unit testy`,
        },
      ],
    });

    console.log(response.message.content);
  } catch (error) {
    console.log("‚ùå CodeLlama nen√≠ sta≈æen√Ω");
    console.log("   St√°hni: ollama pull codellama");
  }
}

async function listModels() {
  console.log("\n" + "=".repeat(60));
  console.log("üìö Available Models\n");

  try {
    const models = await ollama.list();

    console.log("Installed models:");
    models.models.forEach((model) => {
      const sizeGB = (model.size / 1e9).toFixed(2);
      console.log(`   - ${model.name} (${sizeGB} GB)`);
    });

    console.log("\nüí° Doporuƒçen√© modely pro coding:");
    console.log("   - qwen2.5-coder:7b (nejlep≈°√≠ coding model, prosinec 2024)");
    console.log("   - deepseek-coder-v2:16b (excelentn√≠, ale vƒõt≈°√≠)");
    console.log("   - codellama:13b (Meta, osvƒõdƒçen√Ω)");
    console.log("   - llama3.1:8b (obecn√Ω, dobr√Ω na v≈°e)");
  } catch (error) {
    console.log("‚ùå Nelze naƒç√≠st seznam model≈Ø");
  }
}

async function embeddings() {
  console.log("\n" + "=".repeat(60));
  console.log("üî¢ Embeddings Demo\n");

  try {
    const response = await ollama.embeddings({
      model: "llama3.1",
      prompt:
        "function validateEmail(email) { return /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/.test(email); }",
    });

    console.log(
      `Generated embedding with ${response.embedding.length} dimensions`,
    );
    console.log(
      `First 5 values: [${response.embedding.slice(0, 5).join(", ")}...]`,
    );

    console.log("\nüí° Use case: Semantic code search");
    console.log("   1. Generate embeddings pro v≈°echny funkce v codebase");
    console.log("   2. Ulo≈æ do vector DB (Chroma, Pinecone...)");
    console.log("   3. Search by meaning, ne jen keywords");
  } catch (error) {
    console.log("‚ùå Embeddings demo vy≈æaduje bƒõ≈æ√≠c√≠ Ollama");
  }
}

console.log("ü¶ô Ollama - Lok√°ln√≠ LLM made easy\n");
console.log("=".repeat(60));

await basicChat();
await streamingChat();
await codeGeneration();
await listModels();
await embeddings();

console.log("\n" + "=".repeat(60));
console.log("üìä Ollama vs Cloud API\n");
console.log(`
| Aspect        | Ollama (Local)      | Cloud API           |
|---------------|---------------------|---------------------|
| Cost          | Free (po sta≈æen√≠)   | Pay per token       |
| Privacy       | 100% local          | Data sent to cloud  |
| Speed         | Fast (no network)   | Network latency     |
| Model choice  | Limited by RAM      | All models          |
| Quality       | Good (7B-13B)       | Best (GPT-4, Claude)|
| Offline       | ‚úÖ Yes              | ‚ùå No               |
| Setup         | Easy (brew install) | API key only        |
`);

console.log("\nüéØ Kdy pou≈æ√≠t Ollama:");
console.log("   ‚úÖ Prototyping a experimenty");
console.log("   ‚úÖ Citliv√° data (medical, legal...)");
console.log("   ‚úÖ High-volume requests (√∫spora n√°klad≈Ø)");
console.log("   ‚úÖ Offline development");
console.log("   ‚ùå Pot≈ôebuje≈° nejlep≈°√≠ kvalitu (pou≈æij GPT-4/Claude)");
console.log("   ‚ùå Nem√°≈° dostateƒçn√Ω hardware (min 8GB RAM)");
